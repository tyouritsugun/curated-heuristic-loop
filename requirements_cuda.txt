# CHL API Server - NVIDIA CUDA GPU acceleration
#
# This requirements file is for the API server running with NVIDIA CUDA
# GPU acceleration for embeddings and vector search.
#
# Prerequisites:
#   - NVIDIA GPU with CUDA Compute Capability 6.0+ (Pascal or newer)
#   - CUDA Toolkit 12.x installed (e.g., /usr/local/cuda-12.5)
#   - cuDNN libraries
#   - CMake 3.18+
#
# Installation:
#   python -m venv .venv-cuda
#   source .venv-cuda/bin/activate
#   python -m pip install --upgrade pip
#
#   # Set CUDA environment variables for llama-cpp-python build
#   export CUDA_HOME=/usr/local/cuda-12.5
#   export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64:$LD_LIBRARY_PATH
#   export LLAMA_CUBLAS=1
#   export LLAMA_CUDA=1
#   export CMAKE_ARGS="-DGGML_CUDA=on -DLLAMA_CUBLAS=on"
#
#   python -m pip install -r requirements_cuda.txt
#
# Starting the API server:
#   CHL_SEARCH_MODE=gpu python -m uvicorn src.api.server:app --host 127.0.0.1 --port 8000
#
# Note: The MCP server is installed separately via `uv sync` from pyproject.toml
# and communicates with this API server via HTTP.

# Web framework and server
fastapi>=0.114.0
uvicorn[standard]>=0.30.0
python-multipart>=0.0.6
jinja2>=3.1.0

# Database and ORM
sqlalchemy>=2.0.0

# HTTP client for external services
httpx>=0.27.0
requests>=2.31.0

# Google Sheets integration
gspread>=5.0.0
google-auth>=2.0.0
google-auth-oauthlib>=1.0.0

# Data processing and utilities
numpy>=1.24.0,<2.0.0
pyyaml>=6.0
python-dotenv>=1.0.0
pydantic>=2.6.0
tqdm>=4.65.0
tenacity>=8.2.0
markdown>=3.6

# ML / GPU stack (CUDA-accelerated)
faiss-cpu>=1.7.4
huggingface-hub>=0.20.0
sentence-transformers>=3.1.1
# Source install so CUDA flags take effect during build
llama-cpp-python @ git+https://github.com/abetlen/llama-cpp-python.git@v0.3.16
