# CHL API Server - NVIDIA CUDA GPU acceleration
#
# This requirements file is for the API server running with NVIDIA CUDA
# GPU acceleration for embeddings and vector search.
#
# Prerequisites:
#   - NVIDIA GPU with CUDA Compute Capability 6.0+ (Pascal or newer)
#   - NVIDIA drivers with compatible CUDA runtime installed
#
# Installation:
#   python -m venv .venv-cuda
#   source .venv-cuda/bin/activate
#   python -m pip install --upgrade pip
#
#   # Use abetlen's CUDA wheel index for llama-cpp-python (cu124 for CUDA 12.x)
#   PIP_EXTRA_INDEX_URL=https://abetlen.github.io/llama-cpp-python/whl/cu124 \
#     python -m pip install -r requirements_cuda.txt
#
# Starting the API server:
#   python -m uvicorn src.api.server:app --host 127.0.0.1 --port 8000
#   (Backend is automatically configured from data/runtime_config.json)
#
# Note: The MCP server is installed separately via `uv sync` from pyproject.toml
# and communicates with this API server via HTTP.

# Web framework and server
fastapi>=0.114.0
uvicorn[standard]>=0.30.0
python-multipart>=0.0.6
jinja2>=3.1.0

# Database and ORM
sqlalchemy>=2.0.0

# HTTP client for external services
httpx>=0.27.0
requests>=2.31.0

# Google Sheets integration
gspread>=5.0.0
google-auth>=2.0.0
google-auth-oauthlib>=1.0.0

# Data processing and utilities
numpy>=1.24.0,<2.0.0
pyyaml>=6.0
python-dotenv>=1.0.0
pydantic>=2.6.0
tqdm>=4.65.0
tenacity>=8.2.0
markdown>=3.6

# ML / GPU stack (CUDA-accelerated)
# Note: faiss-cpu is sufficient - vector search is fast on CPU
# GPU acceleration is needed for embedding/reranking models (via llama-cpp-python CUDA wheels)
faiss-cpu>=1.7.4
huggingface-hub>=0.20.0
sentence-transformers>=3.1.1
# llama-cpp-python with CUDA support from abetlen's wheel index
# Installed via PIP_EXTRA_INDEX_URL=https://abetlen.github.io/llama-cpp-python/whl/cu124
llama-cpp-python==0.3.16
