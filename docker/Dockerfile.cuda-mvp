# Minimal CUDA 12.5 image that installs the CHL dependencies + llama-cpp-python
# from source and runs the GPU smoke test.
#
# Usage:
#   docker build -f docker/Dockerfile.cuda-mvp -t chl-gpu-mvp .
#   docker run --rm --gpus all chl-gpu-mvp

FROM nvidia/cuda:12.5.0-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    CUDA_HOME=/usr/local/cuda \
    LLAMA_CUDA=1 \
    HF_HUB_ENABLE_HF_TRANSFER=1 \
    GGUF_CACHE=/opt/chl/models

# Install Python 3.11 + build prerequisites
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 python3.11-venv python3.11-distutils python3.11-dev \
    build-essential git curl ca-certificates && \
    rm -rf /var/lib/apt/lists/*

RUN ln -s /usr/bin/python3.11 /usr/local/bin/python && \
    curl -sS https://bootstrap.pypa.io/get-pip.py -o /tmp/get-pip.py && \
    python /tmp/get-pip.py && rm /tmp/get-pip.py

WORKDIR /app

COPY requirements.txt ./requirements.txt
RUN mkdir -p ${GGUF_CACHE}

# Install project dependencies (llama-cpp will compile with CUDA support).
RUN CMAKE_ARGS="-DGGML_CUDA=on -DGGML_CUDA_F16=on" \
    python -m pip install --no-cache-dir -r requirements.txt

COPY scripts/gpu_smoke_test.py ./scripts/gpu_smoke_test.py

CMD ["python", "scripts/gpu_smoke_test.py"]
