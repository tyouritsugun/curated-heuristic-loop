# GPU-enabled environment requirements for the CHL API server.
#
# Usage example:
#   python -m venv .venv-gpu
#   source .venv-gpu/bin/activate
#   python -m pip install --upgrade pip
#   CUDA_HOME=/usr/local/cuda-12.5 \
#   LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64:$LD_LIBRARY_PATH \
#   LLAMA_CUBLAS=1 LLAMA_CUDA=1 \
#   CMAKE_ARGS="-DGGML_CUDA=on -DLLAMA_CUBLAS=on" \
#   python -m pip install -r requirements.txt

fastmcp>=0.3.0
sqlalchemy>=2.0.0
numpy>=1.24.0,<2.0.0
gspread>=5.0.0
google-auth>=2.0.0
google-auth-oauthlib>=1.0.0
tqdm>=4.65.0
pyyaml>=6.0
python-dotenv>=1.0.0
fastapi>=0.114.0
uvicorn[standard]>=0.30.0
httpx>=0.27.0
python-multipart>=0.0.6
pydantic>=2.6.0
jinja2>=3.1.0
tenacity>=8.2.0
markdown>=3.6

# ML / GPU stack
faiss-cpu>=1.7.4
huggingface-hub>=0.20.0
sentence-transformers>=3.1.1
# Source install so CUDA flags take effect during build.
llama-cpp-python @ git+https://github.com/abetlen/llama-cpp-python.git@v0.3.16
