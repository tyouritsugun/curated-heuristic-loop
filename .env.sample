# ==============================================================================
# CHL Environment Configuration
# ==============================================================================
# Copy this file to .env and fill in your values
# .env is gitignored and should never be committed

# ------------------------------------------------------------------------------
# Google Sheets Integration (Required)
# ------------------------------------------------------------------------------
# Path to Google service account JSON credential file
# Create the google service account to make possible for the program to access
# the spreadsheets for import and export.
# Copy the google service account to data/credentials/ folder and set the name here
# Relative paths resolve from project root
GOOGLE_CREDENTIAL_PATH=data/credentials/service-account.json

# Published spreadsheet ID (source for imports)
# Our sample, you may import this and try CHL to see the difference!
IMPORT_SPREADSHEET_ID=1sYfvvsN3AgoKQbfURa0ysgv_h_93u2_zEshCOMzPj2c

# Review spreadsheet ID (target for exports)
EXPORT_SPREADSHEET_ID=1XCa6P2_JL-exUJvaW1F9aHMYRs5yJPzMMlbZQdcDbTk

# ------------------------------------------------------------------------------
# Team Curation Workflow (Semi-Auto Curation)
# ------------------------------------------------------------------------------
# Published/canonical spreadsheet ID (target for curator's publish step)
# This is where the curator publishes the final merged/deduped knowledge base
# Team members import from this sheet to sync to the same baseline
# Can be the same as IMPORT_SPREADSHEET_ID if you want bidirectional sync
PUBLISHED_SPREADSHEET_ID=1sYfvvsN3AgoKQbfURa0ysgv_h_93u2_zEshCOMzPj2c

# Curation database path (temporary DB for merge/dedup workflow)
# Default: data/curation/chl_curation.db
# This database has the same schema as chl.db but is used only during curation
# CURATION_DB_PATH=data/curation/chl_curation.db

# ------------------------------------------------------------------------------
# LLM Access, only set if you plan to run curation.
# ------------------------------------------------------------------------------
# Commercial OpenAI-compatible (e.g., ChatGPT, Gemini):
#   set ONE of: OPENAI_API_KEY, GEMINI_API_KEY, GOOGLE_API_KEY
# Local LM Studio / Ollama (OpenAI-compatible):
#   set LLM_API_KEY (optional if your local endpoint ignores it)
#
# Examples (uncomment the one you use):
# OPENAI_API_KEY=sk-...
# GEMINI_API_KEY=...
# GOOGLE_API_KEY=...
# LLM_API_KEY=local-placeholder


# ------------------------------------------------------------------------------
# Google Sheets Worksheet Names (Optional - Override Defaults)
# ------------------------------------------------------------------------------
# Uncomment to customize if your sheets don't use default tab names
# For your import sheet, the names of the three worksheet names MUST exists 
# IMPORT_WORKSHEET_CATEGORIES=Categories
# IMPORT_WORKSHEET_EXPERIENCES=Experiences
# IMPORT_WORKSHEET_MANUALS=Manuals
# For your export sheet, it will be CLEARED and inserted the worksheets below from your local DB
# EXPORT_WORKSHEET_CATEGORIES=Categories
# EXPORT_WORKSHEET_EXPERIENCES=Experiences
# EXPORT_WORKSHEET_MANUALS=Manuals

# ------------------------------------------------------------------------------
# Skills Feature Toggle (Optional)
# ------------------------------------------------------------------------------
# Enable/disable skill management (read/write/import/export/embeddings).
# Default: true (skills enabled)
# Set to false to run in experience-only mode.
#
# When disabled:
# - MCP: Skill tools not registered; requests return "skills disabled" error
# - API: Skill endpoints return 404
# - Import/Export: Skills sheets skipped (Sheets/Excel/CSV)
# - Embeddings: Skill jobs not processed
# - UI: Skills hidden/disabled with warning
#
# IMPORTANT: Disabling does NOT delete existing skill data.
# Re-enabling restores full functionality immediately.
CHL_SKILLS_ENABLED=true

# ------------------------------------------------------------------------------
# Optional GPU Settings (Advanced - Usually not needed)
# ------------------------------------------------------------------------------
# Number of transformer layers to offload to GPU for GGUF models.
# Smart defaults based on backend (from data/runtime_config.json):
#   - GPU backends (metal/cuda/rocm/arc): -1 (all layers on GPU, best performance)
#   - CPU backend: 0 (models not loaded in CPU mode)
#
# Override only if you have limited VRAM or need to troubleshoot:
#   -1 = All layers on GPU (default for GPU backends, best performance)
#   0  = CPU-only inference (default for CPU backend)
#   N  = First N layers on GPU (for limited VRAM, e.g., 20)
#
# These work for Apple Metal, NVIDIA CUDA, AMD ROCm and Intel Arc backends.
# CHL_EMBEDDING_N_GPU_LAYERS=-1
# CHL_RERANKER_N_GPU_LAYERS=-1

# ------------------------------------------------------------------------------
# Logging
# ------------------------------------------------------------------------------
# MCP server log level for console/file logging. Options: DEBUG, INFO, WARNING,
# ERROR, CRITICAL. Default: INFO.
# CHL_LOG_LEVEL=INFO

# ------------------------------------------------------------------------------
# Hugging Face Hub Configuration
# ------------------------------------------------------------------------------
# Set to 1 to use local cached models only. If models are not yet cached,
# you'll need to download them first with access to Hugging Face.
# HF_HUB_OFFLINE=1
